bots:
# Pathological bots to deny
- import: "(data)/bots/ai-robots-txt.yaml"
- import: "(data)/bots/cloudflare-workers.yaml"
- import: "(data)/bots/headless-browsers.yaml"
- import: "(data)/bots/us-ai-scraper.yaml"

# Search engines to allow
- import: "(data)/crawlers/googlebot.yaml"
- import: "(data)/crawlers/bingbot.yaml"
- import: "(data)/crawlers/duckduckbot.yaml"
- import: "(data)/crawlers/qwantbot.yaml"
- import: "(data)/crawlers/internet-archive.yaml"
- import: "(data)/crawlers/kagibot.yaml"
- import: "(data)/crawlers/marginalia.yaml"
- import: "(data)/crawlers/mojeekbot.yaml"

# Allow common "keeping the internet working" routes (well-known, favicon, robots.txt)
- import: "(data)/common/keep-internet-working.yaml"

# # Punish any bot with "bot" in the user-agent string
# # This is known to have a high false-positive rate, use at your own risk
# - name: generic-bot-catchall
#   user_agent_regex: (?i:bot|crawler)
#   action: CHALLENGE
#   challenge:
#     difficulty: 16  # impossible
#     report_as: 4    # lie to the operator
#     algorithm: slow # intentionally waste CPU cycles and time


# Generic catchall rule
- name: generic-browser
  user_agent_regex: >
    Mozilla|Opera
  action: CHALLENGE

dnsbl: false
